import os
import csv
import datetime
import itertools
import numpy as np
import pandas as pd
from scipy import stats
from scipy.special import gamma, psi


'''Using linear interpolation to fill in missing rows for the period 9:30-16:00 during the trading day'''

def handle(df):
    df.drop(columns=["date"], inplace=True)
    df.set_index("time", inplace=True)
    # Find the date of the missing bar
    if len(df) == 79:
        return df
    while True:
        # Find the time of the missing bar
        foundMissingBar = False
        for i in range(0, len(df) - 1):
            t = datetime.datetime.strptime(df.index[i], "%H:%M:%S")
            subT = datetime.datetime.strptime(df.index[i + 1], "%H:%M:%S") - t
            # in 5-minute increments
            subBar = subT.seconds // 60 // 5
            if subBar == 1:
                continue
            foundMissingBar = True
            buttomIndex = df.index[i]
            topIndex = df.index[i + 1]
            # linear interpolation for generate the missing bar
            for j in range(1, subBar):
                newT = t + datetime.timedelta(minutes=5 * j)
                df.loc[newT.strftime("%H:%M:%S")] = (
                    df.iloc[i] + (df.loc[topIndex] - df.loc[buttomIndex]) * j / subBar
                )
            df.sort_index(inplace=True)
            break
        if not foundMissingBar:
            break
    return df


folder_path = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min1"
new_folder_path = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min2"


for file in os.listdir(folder_path):
    if file.endswith(".csv"):      
        df = pd.read_csv(os.path.join(folder_path, file))
        df = df.groupby("date").apply(handle) 
        df.to_csv(os.path.join(new_folder_path, file))  

# fill it with ffill
template_file =  r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min2\aal.csv"

template_df = pd.read_csv(template_file, usecols=["date", "time"])
source_folder = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min2"
target_folder = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"
if not os.path.exists(target_folder):
    os.mkdir(target_folder)

for file in os.listdir(source_folder):
    if file.endswith(".csv"):
        source_df = pd.read_csv(os.path.join(source_folder, file))
        merged_df = pd.merge(template_df, source_df, on=["date", "time"], how="left")
        filled_df = merged_df.fillna(method="ffill")
        filled_df.to_csv(os.path.join(target_folder, file), index=False)




'''Load all sp500 asset data to calculate normalized features'''


def realized_quantity(fun):
    return intraday_returns.groupby(pd.Grouper(freq="1d")).apply(fun)[index]

if __name__ == "__main__":
    
    folder_path = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"
    for file in os.listdir(folder_path):
        if file.endswith(".csv"):
            data = pd.read_csv(os.path.join(folder_path, file))
            data["date_time"] = pd.to_datetime(data["date"] + " " + data["time"])
            data.set_index("date_time", inplace=True)
            
            # Load data and store the intraday returns
            intraday_returns = np.log(data["close"] / data["close"].shift(1)).dropna() 
            
            # Index of all days
            index = data.groupby(pd.Grouper(freq="1d")).first().dropna().index
            
            # setting & required constants
            M = 48
            # μ1
            mu_1 = np.sqrt((2 / np.pi))
            # μ43
            # mu_43 = 2 ** (2 / 3) * gamma(7 / 6) * gamma(1 / 2) ** (-1)
            
            # First and last price of each day 
            prices_open = data.resample('D').first()['open']
            prices_close = data.resample('D').last()['close']
            
            # Return (close-to-close) 
            r_cc = pd.Series(np.log(prices_close / prices_close.shift(1)), name='r_cc')
            
            # Realized Variance (Andersen and Bollerslev, 1998)
            rv = realized_quantity(lambda x: (x ** 2).sum())
            future_rv = rv.shift(-1)
            
            # Realized absolute variation (Forsberg and Ghysels, 2007)
            rav = mu_1 ** (-1) * M ** (-.5) * realized_quantity(lambda x: x.abs().sum())
            
            # Realized bipower variation (Barndorff-Nielsen and Shephard; 2004, 2006)
            bv = mu_1 ** (-2) * realized_quantity(lambda x: (x.abs() * x.shift(1).abs()).sum())
            
            '''# Standardized tri-power quarticity (see e.g. Forsberg & Ghysels, 2007)
            tq = M * mu_43 ** (-3) * realized_quantity(
                lambda x: (x.abs() ** (4 / 3) * x.shift(1).abs() ** (4 / 3) * x.shift(2).abs() ** (4 / 3)).sum())
            
            # Jump test by Huang and Tauchen (2005)
            j = (np.log(rv) - np.log(bv)) / \
                ((mu_1 ** -4 + 2 * mu_1 ** -2 - 5) / (M * tq * bv ** -2)) ** 0.5
            jump = j.abs() >= stats.norm.ppf(0.999)
            
            # Separate continuous and discontinuous parts of the quadratic variation
            iv = pd.Series(0, index=index)
            iv[jump] = bv[jump] ** 0.5
            iv[~jump] = rv[~jump] ** 0.5
            jv = pd.Series(0, index=index)
            jv[jump] = rv[jump] ** 0.5 - bv[jump] ** 0.5
            jv[jv < 0] = 0'''
            
            # Realized Semivariance (Barndorff-Nielsen, Kinnebrock and Shephard, 2010)
            rv_m = realized_quantity(lambda x: (x ** 2 * (x < 0)).sum())
            rv_p = realized_quantity(lambda x: (x ** 2 * (x > 0)).sum())
            
            '''# Signed jump variation (Patton and Sheppard, 2015)
            sjv = rv_p - rv_m
            sjv_p = sjv * (sjv > 0)
            sjv_m = sjv * (sjv < 0)
            
            # Realized Skewness and Kurtosis  (see, e.g. Amaya, Christoffersen, Jacobs and Vasquez, 2015)
            rm3 = realized_quantity(lambda x: (x ** 3).sum())
            rm4 = realized_quantity(lambda x: (x ** 4).sum())
            rs = np.sqrt(M) * rm3 / rv ** (3 / 2)
            rk = M * rm4 / rv ** 2'''
            
            # Export data
            out = pd.concat([r_cc, rav, rv ** .5, bv ** .5, rv_m ** 0.5, rv_p ** 0.5, future_rv** .5]
                              ,axis=1) # iv, jv, sjv, sjv_p, sjv_m, rs, rk],
            out.columns = ['r_cc', 'rav', 'rvol', 'bvol', 'rvol_m', 'rvol_p','future_RV']
                            # 'ivol', 'jvol', 'sjv', 'sjv_p', 'sjv_m', 'rs', 'rk'
            # df转csv
            out = (out - out.mean()) / out.std()
            out.to_csv(os.path.join(folder_path,  file))   


# delete missing values
#original rows_to_delete = [7,8,14,15,...,]
rows_to_delete = [5,6,12,13,19,20,24,26,27,33,34,40,41,47,48,54,55,56,61,62,63,68,69,75,76,77,82,83,89,90,96,97,103,104,
110,111,112,117,118,124,125,131,132,138,139,145,146,152,153,158,159,160,166,167,173,174,180,181,187,188,194,195,201,202,
208,209,210,215,216,222,223,229,230,231,236,237,243,244,246,250,251,257,258,264,265,271,272,278,279,285,286,292,293,299,
300,306,307,308,313,314,320,321,327,328,334,335,341,342,348,349,355,356,362,363]
folder_path = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"

files = os.listdir(folder_path)
for file in files:
    if file.endswith('.csv'):
        df = pd.read_csv(os.path.join(folder_path, file))
        df = df.drop(rows_to_delete)
        df.to_csv(os.path.join(folder_path, file), index=False)


 # ffill missing values
folder_path =r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"
for file_name in os.listdir(folder_path):
    if file_name.endswith(".csv"):
        file_path = os.path.join(folder_path, file_name)
        # specify index_col=0 to read the first column as a normal column
        df = pd.read_csv(file_path, index_col=0)
        # use ffill method to fill all missing values in both rows and columns
        # specify axis=None to fill along both directions
        df = df.fillna(method="ffill", axis=1) # 修改了这一行
        df = df.fillna(method="bfill", axis=1)
        df.to_csv(file_path, index=False)
        print(f"已处理文件：{file_path}")

# delete the first column
def delete_first_line_in_csv_files(directory):
    for filename in os.listdir(directory):
        if filename.endswith('.csv'):
            with open(os.path.join(directory, filename), 'r') as file:
                lines = file.readlines()
            with open(os.path.join(directory, filename), 'w') as file:
                file.writelines(lines[1:])

delete_first_line_in_csv_files(r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3")



# csv➡npz
folder_path = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"
# 定义一些常量
n_sample = 251
stock_num = 300
feature_num = 6
weight_set = 30
weight_num = 6
operator_set = 5
operator_num = 5
    

data_list = []
for filename in os.listdir(folder_path):
    if filename.endswith(".csv"):
        data = pd.read_csv(os.path.join(folder_path, filename), header=None, index_col=False).values
        data_list.append(data)  

data_array = np.array(data_list).transpose((1, 0, 2))
input_feature = data_array[:, :, :-1]
future_RV = data_array[:, :, -1]
weight_array = np.array([
    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],
    [2/6, 0, 1/6, 1/6, 1/6, 1/6],
    [3/6, 0, 0, 1/6, 1/6, 1/6],
    [4/6, 0, 0, 0, 1/6, 1/6],
    [5/6, 0, 0, 0, 0, 1/6],
    [0, 2/6, 1/6, 1/6, 1/6, 1/6],
    [0, 3/6, 0, 1/6, 1/6, 1/6],
    [0, 4/6, 0, 0, 1/6, 1/6],
    [0, 5/6, 0, 0, 0, 1/6],
    [0, 0, 3/6, 1/6, 1/6, 1/6],
    [0, 0, 4/6, 0, 1/6, 1/6],
    [0, 0, 5/6, 0, 0, 1/6],
    [0, 0, 0, 4/6, 1/6, 1/6],
    [0, 0, 0, 5/6, 0, 1/6],
    [0, 0, 0, 0, 5/6, 1/6],
    [1/6, 2/6, 1/6, 1/6, 0, 1/6],
    [1/6, 3/6, 0, 1/6, 0, 1/6],
    [1/6, 4/6, 0, 0, 0, 1/6],
    [1/6, 0, 2/6, 1/6, 0, 1/6],
    [1/6, 0, 3/6, 0, 0, 1/6],
    [1/6, 0, 0, 2/6, 0, 1/6],
    [1/6, 0, 0, 0, 2/6, 1/6],
    [0, 1/6, 2/6, 1/6, 0, 1/6],
    [0, 1/6, 3/6, 0, 0, 1/6],
    [0, 1/6, 0, 2/6, 0, 1/6],
    [0, 1/6, 0, 0, 2/6, 1/6],
    [0, 0, 1/6, 2/6, 0, 1/6],
    [0, 0, 1/6, 0, 2/6, 1/6],
    [0, 0, 0, 1/6, 2/6, 1/6],
    [1/6, 1/6, 1/6, 1/6, 1/6, 0]
])
input_weight = np.tile(weight_array, (n_sample, 1, 1))
operator_array = np.array(['+', '-', '*', '/', '**']) #  '%'
operator_combinations = itertools.product(operator_array, repeat=operator_num)
operator_combinations = np.array(list(operator_combinations))
input_operator = np.tile(operator_combinations, (n_sample, 1, 1))
np.savez(os.path.join(folder_path, "sp300.npz"), input_feature=input_feature, input_weight=input_weight, input_operator=input_operator, future_RV=future_RV)

# load the npz file and access the arrays
import numpy as np
npzfile = np.load(r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3\sp500_1.npz")
# access the input feature array
input_feature = npzfile['input_feature']
input_feature
print(input_feature.shape)
