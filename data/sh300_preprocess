'''Load all sh300 asset data to calculate normalized features'''


import os
import csv
import itertools
import pandas as pd
import numpy as np
from scipy.special import gamma, psi
from scipy import stats

def realized_quantity(fun):
    return intraday_returns.groupby(pd.Grouper(freq="1d")).apply(fun)[index]

if __name__ == "__main__":
    
    folder_path = r"E:\2023Oct\HTF_risk_factor\data\sh300_1year_1min_23\5m_feature"
    for file in os.listdir(folder_path):
        if file.endswith(".csv"):
            data = pd.read_csv(os.path.join(folder_path, file))
            data["date_time"] = pd.to_datetime(data["date"] + " " + data["time"])
            data.set_index("date_time", inplace=True)
            
            # Load data and store the intraday returns
            intraday_returns = np.log(data["close"] / data["close"].shift(1)).dropna() 
            
            # Index of all days
            index = data.groupby(pd.Grouper(freq="1d")).first().dropna().index
            
            # setting & required constants
            M = 48
            # μ1
            mu_1 = np.sqrt((2 / np.pi))
            # μ43
            # mu_43 = 2 ** (2 / 3) * gamma(7 / 6) * gamma(1 / 2) ** (-1)
            
            # First and last price of each day 
            prices_open = data.resample('D').first()['open']
            prices_close = data.resample('D').last()['close']
            
            # Return (close-to-close) 
            r_cc = pd.Series(np.log(prices_close / prices_close.shift(1)), name='r_cc')
            
            # Realized Variance (Andersen and Bollerslev, 1998)
            rv = realized_quantity(lambda x: (x ** 2).sum())
            
            # Realized absolute variation (Forsberg and Ghysels, 2007)
            rav = mu_1 ** (-1) * M ** (-.5) * realized_quantity(lambda x: x.abs().sum())
            
            # Realized bipower variation (Barndorff-Nielsen and Shephard; 2004, 2006)
            bv = mu_1 ** (-2) * realized_quantity(lambda x: (x.abs() * x.shift(1).abs()).sum())
            
            '''# Standardized tri-power quarticity (see e.g. Forsberg & Ghysels, 2007)
            tq = M * mu_43 ** (-3) * realized_quantity(
                lambda x: (x.abs() ** (4 / 3) * x.shift(1).abs() ** (4 / 3) * x.shift(2).abs() ** (4 / 3)).sum())
            
            # Jump feature by Huang and Tauchen (2005)
            j = (np.log(rv) - np.log(bv)) / \
                ((mu_1 ** -4 + 2 * mu_1 ** -2 - 5) / (M * tq * bv ** -2)) ** 0.5
            jump = j.abs() >= stats.norm.ppf(0.999)
            
            # Separate continuous and discontinuous parts of the quadratic variation
            iv = pd.Series(0, index=index)
            iv[jump] = bv[jump] ** 0.5
            iv[~jump] = rv[~jump] ** 0.5
            jv = pd.Series(0, index=index)
            jv[jump] = rv[jump] ** 0.5 - bv[jump] ** 0.5
            jv[jv < 0] = 0'''
            
            # Realized Semivariance (Barndorff-Nielsen, Kinnebrock and Shephard, 2010)
            rv_m = realized_quantity(lambda x: (x ** 2 * (x < 0)).sum())
            rv_p = realized_quantity(lambda x: (x ** 2 * (x > 0)).sum())
            
            '''# Signed jump variation (Patton and Sheppard, 2015)
            sjv = rv_p - rv_m
            sjv_p = sjv * (sjv > 0)
            sjv_m = sjv * (sjv < 0)
            
            # Realized Skewness and Kurtosis  (see, e.g. Amaya, Christoffersen, Jacobs and Vasquez, 2015)
            rm3 = realized_quantity(lambda x: (x ** 3).sum())
            rm4 = realized_quantity(lambda x: (x ** 4).sum())
            rs = np.sqrt(M) * rm3 / rv ** (3 / 2)
            rk = M * rm4 / rv ** 2'''
            
            # Export data
            out = pd.concat([r_cc, rav, rv ** .5, bv ** .5, rv_m ** 0.5, rv_p ** 0.5]
                              ,axis=1) # iv, jv, sjv, sjv_p, sjv_m, rs, rk],
            out.columns = ['r_cc', 'rav', 'rvol', 'bvol', 'rvol_m', 'rvol_p']
                            # 'ivol', 'jvol', 'sjv', 'sjv_p', 'sjv_m', 'rs', 'rk'
            # df转csv
            out = (out - out.mean()) / out.std()
            out.to_csv(os.path.join(folder_path,  file)) 


# Remove empty lines and forward fill missing values
drop_rows = [7,8,14,15,21,22,28,29,35,36,42,43,49,50,56,57,63,64,65,70,71,78,77,84,85,86,87,88,89,90,91,92,98,99,105,106,112,113,
119,120,126,127,133,134,140,141,147,148,154,155,158,161,162,168,169,175,176,182,183,184,185,186,189,190,196,197,203,204,210,211,
217,218,224,225,231,232,236,237,238,239,245,246,252,253,259,260,266,267,273,274,280,281,287,288,294,295,301,302,308,309,315,316,
322,323,329,330,335,336,337,338,339,340,341,342,343,344,350,351,357,358,364,365]
folder_path = r"E:\2023Oct\HTF_risk_factor\data\sh300_1year_1min_23\5m_feature"
for file_name in os.listdir(folder_path):
    if file_name.endswith(".csv"):
        file_path = os.path.join(folder_path, file_name)
        df = pd.read_csv(file_path)
        df = df.drop(drop_rows, axis=0)
        df = df.fillna(method="ffill")
        df = df.fillna(method="bfill")
        df.to_csv(file_path, index=False)
        print(f"已处理文件：{file_path}")
 

# ffill missing values
import os
import pandas as pd

folder_path =r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"
for file_name in os.listdir(folder_path):
    if file_name.endswith(".csv"):
        file_path = os.path.join(folder_path, file_name)
        # specify index_col=0 to read the first column as a normal column
        df = pd.read_csv(file_path, index_col=0)
        # use ffill method to fill all missing values in both rows and columns
        # specify axis=None to fill along both directions
        df = df.fillna(method="ffill", axis=1) # 修改了这一行
        df = df.fillna(method="bfill", axis=1)
        df.to_csv(file_path, index=False)
        print(f"已处理文件：{file_path}")


# delete first column
def delete_first_line_in_csv_files(directory):
    for filename in os.listdir(directory):
        if filename.endswith('.csv'):
            with open(os.path.join(directory, filename), 'r') as file:
                lines = file.readlines()
            with open(os.path.join(directory, filename), 'w') as file:
                file.writelines(lines[1:])




# csv➡npz
folder_path = r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3"

n_sample = 251
stock_num = 300
feature_num = 6
weight_set = 30
weight_num = 6
operator_set = 5
operator_num = 5
    
data_list = []
for filename in os.listdir(folder_path):
    if filename.endswith(".csv"):
        data = np.loadtxt(os.path.join(folder_path, filename), delimiter=",")
        data_list.append(data)
    
data_array = np.array(data_list).transpose((1, 0, 2))
input_feature = data_array[:, :, :-1]
future_RV = data_array[:, :, -1]
weight_array = np.array([
    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],
    [2/6, 0, 1/6, 1/6, 1/6, 1/6],
    [3/6, 0, 0, 1/6, 1/6, 1/6],
    [4/6, 0, 0, 0, 1/6, 1/6],
    [5/6, 0, 0, 0, 0, 1/6],
    [0, 2/6, 1/6, 1/6, 1/6, 1/6],
    [0, 3/6, 0, 1/6, 1/6, 1/6],
    [0, 4/6, 0, 0, 1/6, 1/6],
    [0, 5/6, 0, 0, 0, 1/6],
    [0, 0, 3/6, 1/6, 1/6, 1/6],
    [0, 0, 4/6, 0, 1/6, 1/6],
    [0, 0, 5/6, 0, 0, 1/6],
    [0, 0, 0, 4/6, 1/6, 1/6],
    [0, 0, 0, 5/6, 0, 1/6],
    [0, 0, 0, 0, 5/6, 1/6],
    [1/6, 2/6, 1/6, 1/6, 0, 1/6],
    [1/6, 3/6, 0, 1/6, 0, 1/6],
    [1/6, 4/6, 0, 0, 0, 1/6],
    [1/6, 0, 2/6, 1/6, 0, 1/6],
    [1/6, 0, 3/6, 0, 0, 1/6],
    [1/6, 0, 0, 2/6, 0, 1/6],
    [1/6, 0, 0, 0, 2/6, 1/6],
    [0, 1/6, 2/6, 1/6, 0, 1/6],
    [0, 1/6, 3/6, 0, 0, 1/6],
    [0, 1/6, 0, 2/6, 0, 1/6],
    [0, 1/6, 0, 0, 2/6, 1/6],
    [0, 0, 1/6, 2/6, 0, 1/6],
    [0, 0, 1/6, 0, 2/6, 1/6],
    [0, 0, 0, 1/6, 2/6, 1/6],
    [1/6, 1/6, 1/6, 1/6, 1/6, 0]
])
input_weight = np.tile(weight_array, (n_sample, 1, 1))
operator_array = np.array(['+', '-', '*', '/', '**']) #  '%'
operator_combinations = itertools.product(operator_array, repeat=operator_num)
operator_combinations = np.array(list(operator_combinations))
input_operator = np.tile(operator_combinations, (n_sample, 1, 1))
np.savez(os.path.join(folder_path, "sp300.npz"), input_feature=input_feature, input_weight=input_weight, input_operator=input_operator, future_RV=future_RV)
delete_first_line_in_csv_files(r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3")

# load the npz file and access the arrays
import numpy as np
npzfile = np.load(r"E:\2023Oct\HTF_risk_factor\data\sp500_1year_5min_23\5_min3\sp500_1.npz")
# access the input feature array
input_feature = npzfile['input_feature']
input_feature
print(input_feature.shape)
